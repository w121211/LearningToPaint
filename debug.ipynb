{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(1000, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (9): ReLU(inplace=True)\n",
      "  (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (12): ReLU(inplace=True)\n",
      "  (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (16): ReLU(inplace=True)\n",
      "  (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (19): ReLU(inplace=True)\n",
      "  (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (26): ReLU(inplace=True)\n",
      "  (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (32): ReLU(inplace=True)\n",
      "  (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (36): ReLU(inplace=True)\n",
      "  (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (39): ReLU(inplace=True)\n",
      "  (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (42): ReLU(inplace=True)\n",
      "  (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "torch.Size([1, 512, 13, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 5, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "m = models.vgg16_bn(pretrained=True).features\n",
    "print(m)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "x = transform(Image.open(\"/workspace/CoordConv-pytorch/data/pin/0189dc019d927ba127ac8528707d445e.jpg\"))\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "# y = vgg(x)\n",
    "y = m(x)\n",
    "print(y.shape)\n",
    "y = F.avg_pool2d(y, 3, padding=1)\n",
    "y.shape\n",
    "\n",
    "# y.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.144625 , 5.944611 , 6.1140833, 6.1475325, 5.9054418, 5.822255 ,\n",
       "        5.593646 , 6.059982 , 6.483781 , 5.889703 , 6.2198386, 5.6442757,\n",
       "        5.4158716, 6.0971427, 6.301312 , 5.8527713, 6.0597453, 5.635212 ,\n",
       "        5.7684083, 6.2265472, 6.2389145, 5.859596 , 6.095941 , 5.935408 ,\n",
       "        6.0333223, 6.215909 , 5.8792686, 5.91635  , 5.9047155, 6.205185 ,\n",
       "        5.846923 , 6.233918 , 5.5446906, 5.8319316, 5.9259877, 5.958965 ,\n",
       "        5.628144 , 5.866509 , 5.9294066, 6.2537155, 5.896252 , 5.4413447,\n",
       "        5.939777 , 6.3528495, 5.5589347, 5.679134 , 5.9253144, 5.956631 ,\n",
       "        5.4057198, 5.689386 , 6.215873 , 6.3554225, 5.7686996, 5.8904533,\n",
       "        6.1087456, 6.1338105, 5.7976418, 6.0996213, 6.068554 , 5.8629956,\n",
       "        5.54     , 5.9927263, 6.0861096, 5.435902 , 5.8434224, 6.0457845,\n",
       "        6.111389 , 5.9799685, 5.864781 , 6.088475 , 5.8947473, 6.010778 ,\n",
       "        5.9563384, 5.7499785, 6.049435 , 5.5257716, 6.036753 , 5.9806967,\n",
       "        6.473262 , 5.7111087, 6.148232 , 5.85405  , 6.006406 , 5.775173 ,\n",
       "        5.983325 , 5.6823964, 5.8851924, 5.825068 , 5.8734937, 6.02342  ,\n",
       "        6.3669105, 5.691472 , 5.7399282, 5.9059134, 5.6796103, 6.1894426,\n",
       "        5.7870684, 5.891313 , 5.908944 , 6.325535 , 5.352935 , 5.858054 ,\n",
       "        6.2601647, 6.229171 , 5.99441  , 5.831111 , 5.989212 , 6.233527 ,\n",
       "        5.783399 , 6.3520665, 5.834124 , 5.995937 , 6.1512227, 6.05993  ,\n",
       "        6.002011 , 6.378929 , 6.2854433, 5.9114013, 6.092896 , 5.964631 ,\n",
       "        6.122557 , 6.0303144, 6.110331 , 6.036285 , 6.418102 , 5.919666 ,\n",
       "        6.0839834, 6.362044 , 5.9751134, 5.580715 , 6.469125 , 5.866593 ,\n",
       "        6.099418 , 6.366398 , 5.884028 , 5.973046 , 5.8967214, 6.286158 ,\n",
       "        5.487846 , 6.119428 , 5.8266134, 5.9084635, 5.8046556, 5.802289 ,\n",
       "        6.3340077, 6.118488 , 5.774956 , 5.929862 , 5.817696 , 5.9366736,\n",
       "        6.1417046, 6.242096 , 5.9180865, 6.075536 , 6.0816865, 6.151443 ,\n",
       "        6.095158 , 5.654719 , 6.10598  , 6.127931 , 5.901671 , 5.977088 ,\n",
       "        5.8685203, 5.70904  , 6.0953817, 5.849187 , 6.0875983, 6.2894444,\n",
       "        5.691643 , 5.984596 , 5.656511 , 6.068234 , 5.8117876, 5.802542 ,\n",
       "        6.2743263, 6.0907974, 6.019857 , 5.7959814, 5.6869407, 5.6244974,\n",
       "        5.312457 , 5.807591 , 6.320768 , 6.113541 , 5.815972 , 5.7877617,\n",
       "        5.8710794, 6.088735 , 5.909988 , 5.8748384, 6.1304045, 5.837806 ,\n",
       "        5.9654374, 5.9299846, 5.827675 , 6.4261675, 5.910466 , 5.9395137,\n",
       "        6.2161407, 6.051952 ],\n",
       "       [5.925822 , 6.0250216, 5.987419 , 6.0188284, 5.968109 , 5.998589 ,\n",
       "        5.9598765, 6.2040477, 5.80858  , 5.994913 , 6.086571 , 6.0240254,\n",
       "        6.132747 , 5.972135 , 5.978472 , 5.9702964, 5.9050198, 6.0817327,\n",
       "        5.919136 , 6.0409236, 6.0254054, 6.107094 , 5.7149596, 6.0097446,\n",
       "        5.902525 , 5.838207 , 6.190602 , 6.1729565, 5.8621335, 6.07255  ,\n",
       "        5.9972982, 5.9534454, 5.861658 , 6.052211 , 6.1278934, 6.1229134,\n",
       "        6.1375875, 6.210554 , 6.073081 , 6.0077233, 5.9677997, 6.10258  ,\n",
       "        5.7703385, 5.9145207, 6.122921 , 6.200295 , 6.118893 , 6.0386024,\n",
       "        6.2657356, 5.958557 , 6.057783 , 5.8807735, 6.12257  , 6.064142 ,\n",
       "        5.947653 , 6.121315 , 5.8014894, 5.8363647, 5.9433036, 5.8877735,\n",
       "        5.968534 , 5.981237 , 6.057463 , 6.1180587, 5.867554 , 6.267336 ,\n",
       "        5.9407463, 6.146467 , 5.874928 , 6.044787 , 5.8125157, 6.020309 ,\n",
       "        5.9814763, 6.1095643, 6.2239666, 6.0621343, 6.049425 , 5.9412947,\n",
       "        6.057963 , 6.0018806, 6.1450043, 5.9110074, 5.9311438, 5.963685 ,\n",
       "        6.11968  , 6.01225  , 5.942906 , 6.2821217, 5.8131557, 5.943982 ,\n",
       "        5.9773884, 6.027362 , 6.1523466, 5.8934426, 5.9963675, 5.76025  ,\n",
       "        5.8651586, 5.842514 , 5.947698 , 5.6604733, 6.0108128, 5.999732 ,\n",
       "        5.813177 , 5.742568 , 5.8745112, 6.462166 , 5.9565983, 5.975623 ,\n",
       "        6.1227922, 5.8109303, 6.0266533, 5.9794836, 5.9720626, 5.975086 ,\n",
       "        6.2474976, 5.9311967, 5.979717 , 6.0469394, 6.004518 , 5.910835 ,\n",
       "        5.885701 , 5.965836 , 6.1749434, 6.068988 , 5.9969044, 5.9759674,\n",
       "        5.874061 , 5.937723 , 5.8362327, 6.0855503, 5.770035 , 6.061533 ,\n",
       "        6.037607 , 6.043361 , 6.021537 , 6.099609 , 6.0070786, 5.9880524,\n",
       "        5.972224 , 5.8994403, 6.01223  , 6.030153 , 5.961976 , 6.147959 ,\n",
       "        5.873777 , 5.9434795, 6.1645856, 6.1040673, 6.0236216, 6.2026944,\n",
       "        5.811121 , 6.0587406, 5.9913387, 5.983846 , 5.815632 , 6.077899 ,\n",
       "        5.788257 , 6.181998 , 5.6589665, 5.948967 , 5.975233 , 6.058056 ,\n",
       "        5.7450275, 5.9758053, 6.0918884, 6.103446 , 6.219438 , 5.8260064,\n",
       "        6.04741  , 5.8065925, 5.884613 , 5.8531804, 6.0963387, 5.9339986,\n",
       "        6.1761827, 5.905922 , 5.9982834, 5.983006 , 6.1594396, 6.058915 ,\n",
       "        5.885786 , 6.0215335, 5.845445 , 5.9271502, 6.0889683, 6.1533637,\n",
       "        6.01812  , 5.924179 , 6.038254 , 5.874859 , 5.845518 , 6.081906 ,\n",
       "        5.994164 , 5.9123125, 5.9235053, 6.039032 , 5.9604573, 5.97968  ,\n",
       "        5.909524 , 6.020275 ],\n",
       "       [1.4638383, 1.6008306, 1.4416549, 1.5386672, 1.4861155, 1.6525192,\n",
       "        1.566028 , 1.5013852, 1.5562848, 1.3767012, 1.486114 , 1.4296112,\n",
       "        1.6113912, 1.3708675, 1.4937111, 1.4970627, 1.4286777, 1.5830314,\n",
       "        1.433464 , 1.5290097, 1.5352104, 1.3648502, 1.6114511, 1.3287396,\n",
       "        1.5700341, 1.4999238, 1.5780104, 1.3612988, 1.3942794, 1.5010002,\n",
       "        1.5391086, 1.5319861, 1.5042725, 1.460436 , 1.5595212, 1.5067875,\n",
       "        1.5753257, 1.6190447, 1.4881105, 1.3968527, 1.6002781, 1.5634964,\n",
       "        1.4825484, 1.500069 , 1.4380263, 1.5989356, 1.4060024, 1.5050216,\n",
       "        1.6568625, 1.4386541, 1.609343 , 1.5000948, 1.5233603, 1.5119581,\n",
       "        1.3820053, 1.5564587, 1.4901668, 1.4687076, 1.501734 , 1.454608 ,\n",
       "        1.5293771, 1.542573 , 1.4951799, 1.5218052, 1.4562132, 1.4567375,\n",
       "        1.4872947, 1.5039015, 1.5926292, 1.4835149, 1.4111115, 1.4654698,\n",
       "        1.4343768, 1.3681474, 1.5510657, 1.4135226, 1.5189543, 1.4757984,\n",
       "        1.4689709, 1.5638036, 1.4864746, 1.6483831, 1.5126393, 1.5813664,\n",
       "        1.6337084, 1.4984927, 1.6465378, 1.404782 , 1.4140462, 1.4444246,\n",
       "        1.4809177, 1.5609888, 1.530134 , 1.5096173, 1.4988574, 1.5421083,\n",
       "        1.4388826, 1.4546065, 1.6089342, 1.4173541, 1.4621845, 1.5435231,\n",
       "        1.498854 , 1.433864 , 1.521098 , 1.5510777, 1.5622857, 1.4771569,\n",
       "        1.567854 , 1.5406746, 1.5779586, 1.5355182, 1.4308444, 1.5991142,\n",
       "        1.5141147, 1.5017928, 1.4002706, 1.606781 , 1.4583771, 1.5344087,\n",
       "        1.5184753, 1.4795237, 1.4742085, 1.5154786, 1.5527284, 1.3516177,\n",
       "        1.5648044, 1.5121522, 1.4315057, 1.5022392, 1.3909553, 1.583673 ,\n",
       "        1.5500723, 1.3751308, 1.5159603, 1.4068074, 1.5674187, 1.5743446,\n",
       "        1.480009 , 1.5066628, 1.5123576, 1.3968995, 1.513866 , 1.5679222,\n",
       "        1.5073836, 1.4238354, 1.4579729, 1.5352663, 1.4815757, 1.4344276,\n",
       "        1.5207815, 1.538737 , 1.5715609, 1.407332 , 1.4931663, 1.5214248,\n",
       "        1.5094731, 1.488424 , 1.4541479, 1.5803059, 1.5707824, 1.5779755,\n",
       "        1.5662633, 1.5072528, 1.4881831, 1.5212065, 1.6140201, 1.4035078,\n",
       "        1.484341 , 1.5340409, 1.582515 , 1.5146102, 1.5209758, 1.5102481,\n",
       "        1.4348319, 1.4359585, 1.5205846, 1.5988021, 1.5855075, 1.4952798,\n",
       "        1.5260478, 1.401501 , 1.4220265, 1.555708 , 1.5089825, 1.4219034,\n",
       "        1.5579702, 1.5041429, 1.4931298, 1.4765903, 1.4804479, 1.5067968,\n",
       "        1.3801796, 1.4343332, 1.4237714, 1.6014262, 1.5613261, 1.508968 ,\n",
       "        1.4665829, 1.3476727]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pairwise_distance(data1, data2=None, device=-1):\n",
    "\tr'''\n",
    "\tusing broadcast mechanism to calculate pairwise ecludian distance of data\n",
    "\tthe input data is N*M matrix, where M is the dimension\n",
    "\twe first expand the N*M matrix into N*1*M matrix A and 1*N*M matrix B\n",
    "\tthen a simple elementwise operation of A and B will handle the pairwise operation of points represented by data\n",
    "\t'''\n",
    "\tif data2 is None:\n",
    "\t\tdata2 = data1 \n",
    "\n",
    "\tif device!=-1:\n",
    "\t\tdata1, data2 = data1.cuda(device), data2.cuda(device)\n",
    "\n",
    "\t#N*1*M\n",
    "\tA = data1.unsqueeze(dim=1)\n",
    "\n",
    "\t#1*N*M\n",
    "\tB = data2.unsqueeze(dim=0)\n",
    "\n",
    "\tdis = (A-B)**2.0\n",
    "\t#return N*N matrix for pairwise distance\n",
    "\tdis = dis.sum(dim=-1).squeeze()\n",
    "\treturn dis\n",
    "\n",
    "\n",
    "def forgy(X, n_clusters):\n",
    "\t_len = len(X)\n",
    "\tindices = np.random.choice(_len, n_clusters)\n",
    "\tinitial_state = X[indices]\n",
    "\treturn initial_state\n",
    "\n",
    "\n",
    "def lloyd(X, n_clusters, device=0, tol=1e-4):\n",
    "\tX = torch.from_numpy(X).float()\n",
    "#     .cuda(device)\n",
    "\n",
    "\tinitial_state = forgy(X, n_clusters)\n",
    "\n",
    "\n",
    "\twhile True:\n",
    "\t\tdis = pairwise_distance(X, initial_state)\n",
    "\n",
    "\t\tchoice_cluster = torch.argmin(dis, dim=1)\n",
    "\n",
    "\t\tinitial_state_pre = initial_state.clone()\n",
    "\n",
    "\t\tfor index in range(n_clusters):\n",
    "\t\t\tselected = torch.nonzero(choice_cluster==index).squeeze()\n",
    "\n",
    "\t\t\tselected = torch.index_select(X, 0, selected)\n",
    "\t\t\tinitial_state[index] = selected.mean(dim=0)\n",
    "\t\t\n",
    "\n",
    "\t\tcenter_shift = torch.sum(torch.sqrt(torch.sum((initial_state - initial_state_pre) ** 2, dim=1)))\n",
    "\n",
    "\t\tif center_shift ** 2 < tol:\n",
    "\t\t\tbreak\n",
    "\n",
    "\treturn choice_cluster.cpu().numpy(), initial_state.cpu().numpy()\n",
    "\n",
    "A = np.concatenate([np.random.randn(100, 200), np.random.randn(100, 200)+3, np.random.randn(100, 200)+6], axis=0)\n",
    "\n",
    "#lloyd(X, n_clusters, device=0, tol=1e-4)\n",
    "clusters_index, centers = lloyd(A, 3, device=-1, tol=1e-4)\n",
    "clusters_index\n",
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 2, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y[0].shape\n",
    "import torch.nn.functional as F\n",
    "y.shape\n",
    "# y = F.avg_pool2d(y, (3, 3), padding=1)\n",
    "# y.view(1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1.5332396  0.         1.1868494  ... 0.         0.06440655 0.        ]], shape=(1, 1280), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q -U tensorflow_hub\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "def load_img(path_to_img):\n",
    "  max_dim = 512\n",
    "  img = tf.io.read_file(path_to_img)\n",
    "  img = tf.image.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "  long_dim = max(shape)\n",
    "  scale = max_dim / long_dim\n",
    "\n",
    "  new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "  img = tf.image.resize(img, new_shape)\n",
    "  img = img[tf.newaxis, :]\n",
    "  return img\n",
    "\n",
    "\n",
    "content_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
    "content_image = load_img(content_path)\n",
    "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
    "x = tf.image.resize(x, (224, 224))\n",
    "\n",
    "feature_extractor_url = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2\" #@param {type:\"string\"}\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_url,\n",
    "                                         input_shape=(224,224,3))\n",
    "feature_batch = feature_extractor_layer(x)\n",
    "\n",
    "print(feature_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "  tensor = tensor*255\n",
    "  tensor = np.array(tensor, dtype=np.uint8)\n",
    "  if np.ndim(tensor)>3:\n",
    "    assert tensor.shape[0] == 1\n",
    "    tensor = tensor[0]\n",
    "  return PIL.Image.fromarray(tensor)\n",
    "\n",
    "\n",
    "def load_img(path_to_img):\n",
    "  max_dim = 512\n",
    "  img = tf.io.read_file(path_to_img)\n",
    "  img = tf.image.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "  long_dim = max(shape)\n",
    "  scale = max_dim / long_dim\n",
    "\n",
    "  new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "  img = tf.image.resize(img, new_shape)\n",
    "  img = img[tf.newaxis, :]\n",
    "  return img\n",
    "\n",
    "\n",
    "content_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
    "# https://commons.wikimedia.org/wiki/File:Vassily_Kandinsky,_1913_-_Composition_7.jpg\n",
    "# style_path = tf.keras.utils.get_file('kandinsky5.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')\n",
    "\n",
    "content_image = load_img(content_path)\n",
    "\n",
    "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
    "x = tf.image.resize(x, (224, 224))\n",
    "# vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
    "# prediction_probabilities = vgg(x)\n",
    "# prediction_probabilities.shape\n",
    "\n",
    "# predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n",
    "# [(class_name, prob) for (number, class_name, prob) in predicted_top_5]\n",
    "\n",
    "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = vgg(x)\n",
    "# y = tf.keras.layers.Flatten(y)\n",
    "y.shape\n",
    "\n",
    "# for layer in vgg.layers:\n",
    "#   print(layer.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reward(xy_a: np.array, xy_b: np.array):\n",
    "    dist = np.linalg.norm(xy_a - xy_b, axis=1)\n",
    "    r = -1 * dist / 2 + 1\n",
    "    r = np.clip(r, -1, None)\n",
    "    r = np.sum(r)\n",
    "#     elif r > 0:\n",
    "#         r *= 0.05 ** self.cur_step  # 衰退因子\n",
    "    return r\n",
    "\n",
    "a, b = np.random.rand(2,4), np.random.rand(2,4)\n",
    "print(_reward(a, b))\n",
    "# _reward(a[0], b[0]) + _reward(a[1], b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "%cd /workspace/LearningToPaint/rllib\n",
    "\n",
    "from canvas_env import CanvasEnv\n",
    "\n",
    "e = CanvasEnv({})\n",
    "obs = e.reset()\n",
    "\n",
    "def to_image(array):\n",
    "    im = (array * 255).astype(np.uint8)\n",
    "    im = np.squeeze(im, 2)\n",
    "    return Image.fromarray(im)\n",
    "\n",
    "acc = 0\n",
    "for _ in range(5):\n",
    "    obs, reward, done, _ = e.step([3, 3])\n",
    "    acc += reward\n",
    "    if done:\n",
    "        break\n",
    "acc\n",
    "\n",
    "# to_image(obs['target_im'])\n",
    "# to_image(obs['cur_im'])\n",
    "# im = obs['cur_im']\n",
    "# obs['obj_status']\n",
    "\n",
    "# _obs['target_im'] == obs['target_im']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, (b, c) = [1, np.array([2,3])]\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "im = Image.new(\"L\", (7, 10))\n",
    "draw = ImageDraw.Draw(im)\n",
    "draw.rectangle([2, 2, 4, 4], fill=255)\n",
    "\n",
    "# np.array(im).shape\n",
    "# tf.expand_dims(tf.image.convert_image_dtype(np.array(im), tf.float32), 2)\n",
    "# x = np.array([[1]])\n",
    "# x[0, 0]\n",
    "\n",
    "# tf.keras.preprocessing.image.img_to_array(im).shape\n",
    "# np.array(im)\n",
    "x = np.array(im)\n",
    "x = np.expand_dims(x, -1)\n",
    "x = np.expand_dims(x, 0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = tf.random.normal((1,2,3,4))\n",
    "a = np.array([1,2,3,4])\n",
    "# x.shape == self.shape and np.all(x >= self.low) and np.all(x <= self.high)\n",
    "# def at():\n",
    "#     print('at')\n",
    "# def af():\n",
    "#     print('af')\n",
    "# # def a: print('a')\n",
    "# tf.cond(np.all(a >= 1), af, af )\n",
    "\n",
    "np.all(a>=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3])\n",
    "x * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gym import spaces\n",
    "# from ray.rllib.models.catalog import ModelCatalog\n",
    "\n",
    "# # DICT_SPACE = spaces.Tuple([\n",
    "# #     spaces.Discrete(100),\n",
    "# #     spaces.Box(low=-100, high=100, shape=(3, ))\n",
    "# # ])\n",
    "\n",
    "# a = spaces.Box(low=-100, high=100, shape=(3,))\n",
    "# _dist_class, logit_dim = ModelCatalog.get_action_dist(a, None)\n",
    "# print(_dist_class, logit_dim)\n",
    "\n",
    "# tf.random.categorical(tf.math.log([[10., 10.]]), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "num_outputs = 1 + 2*2 # logits for num_objects (one each) & (mu, sdd) for coordinates (x0, y0) each\n",
    "\n",
    "# define a cnn model\n",
    "input_img = Input(shape=(9, 9, 1))  # (H, W, C)\n",
    "x = Conv2D(8, 3, activation=\"relu\", padding=\"same\")(input_img)\n",
    "x = Conv2D(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = Conv2D(64, 3, padding=\"same\")(x)\n",
    "x = Conv2D(1, 3, padding=\"same\")(x)\n",
    "out = Flatten()(x)\n",
    "\n",
    "cnn_model= Model(input_img, out)\n",
    "cnn_model.summary()\n",
    "\n",
    "cur_im = Input(shape=(9, 9, 1))\n",
    "target_im = Input(shape=(9, 9, 1))\n",
    "\n",
    "# The vision model will be shared, weights and all\n",
    "out_cur = cnn_model(cur_im)\n",
    "out_target = cnn_model(target_im)\n",
    "obj_status = Input(shape=(1, 4))  # (num_obj, 4=(x0, y0, x1, y1))\n",
    "\n",
    "x = Flatten()(obj_status)\n",
    "x = tf.keras.layers.concatenate([out_cur, out_target, x])\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "out = Dense(num_outputs, activation=None)(x)\n",
    "\n",
    "model = Model([cur_im, target_im, obj_status], out)\n",
    "\n",
    "model([tf.random.normal((10, 9, 9, 1)), tf.random.normal((10, 9, 9, 1)), tf.random.normal((10, 1, 4))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "# m = Categorical(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "m = Categorical(torch.tensor([ 0.1, 0.25, 0.25, 0.25 ]))\n",
    "a = m.sample()\n",
    "print(a)\n",
    "m.log_prob(a)\n",
    "\n",
    "# from gym import error, spaces\n",
    "\n",
    "# x = spaces.Tuple(\n",
    "#             (spaces.Discrete(1), spaces.Box(low=0, high=1, shape=(1,)))\n",
    "#         )\n",
    "# for s in x:\n",
    "#     print(s)\n",
    "\n",
    "\n",
    "# a = torch.rand(1, 3, 3)\n",
    "# b = torch.rand(1, 3, 3)\n",
    "# c = torch.rand(1, 4)\n",
    "# print(a, b, c)\n",
    "\n",
    "# z = torch.cat((a.view(-1), b.view(-1), c.view(-1)), dim=0)\n",
    "# a, b = torch.split(z, 2*3*3, dim=0)\n",
    "# print(a.view(2,3,3), b.view(1,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FixedNormal = torch.distributions.Normal\n",
    "\n",
    "def init(module, weight_init, bias_init, gain=1):\n",
    "    weight_init(module.weight.data, gain=gain)\n",
    "    bias_init(module.bias.data)\n",
    "    return module\n",
    "\n",
    "class AddBias(nn.Module):\n",
    "    def __init__(self, bias):\n",
    "        super(AddBias, self).__init__()\n",
    "        self._bias = nn.Parameter(bias.unsqueeze(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            bias = self._bias.t().view(1, -1)\n",
    "        else:\n",
    "            bias = self._bias.t().view(1, -1, 1, 1)\n",
    "\n",
    "        return x + bias\n",
    "\n",
    "class DiagGaussian(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(DiagGaussian, self).__init__()\n",
    "\n",
    "        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "                               constant_(x, 0))\n",
    "\n",
    "        self.fc_mean = init_(nn.Linear(num_inputs, num_outputs))\n",
    "        self.logstd = AddBias(torch.zeros(num_outputs))\n",
    "\n",
    "    def forward(self, x):\n",
    "        action_mean = self.fc_mean(x)\n",
    "\n",
    "        #  An ugly hack for my KFAC implementation.\n",
    "        zeros = torch.zeros(action_mean.size())\n",
    "        if x.is_cuda:\n",
    "            zeros = zeros.cuda()\n",
    "\n",
    "        action_logstd = self.logstd(zeros)\n",
    "        print(action_mean, action_logstd.exp())\n",
    "        return FixedNormal(action_mean, action_logstd.exp())\n",
    "\n",
    "m = DiagGaussian(512, 2)\n",
    "x = torch.rand((1, 512))\n",
    "dist = m(x)\n",
    "dist.sample()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NNBase(nn.Module):\n",
    "    def __init__(self, recurrent, recurrent_input_size, hidden_size):\n",
    "        super(NNBase, self).__init__()\n",
    "\n",
    "        self._hidden_size = hidden_size\n",
    "        self._recurrent = recurrent\n",
    "\n",
    "        if recurrent:\n",
    "            self.gru = nn.GRU(recurrent_input_size, hidden_size)\n",
    "            for name, param in self.gru.named_parameters():\n",
    "                if 'bias' in name:\n",
    "                    nn.init.constant_(param, 0)\n",
    "                elif 'weight' in name:\n",
    "                    nn.init.orthogonal_(param)\n",
    "\n",
    "    @property\n",
    "    def is_recurrent(self):\n",
    "        return self._recurrent\n",
    "\n",
    "    @property\n",
    "    def recurrent_hidden_state_size(self):\n",
    "        if self._recurrent:\n",
    "            return self._hidden_size\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._hidden_size\n",
    "\n",
    "    def _forward_gru(self, x, hxs, masks):\n",
    "        if x.size(0) == hxs.size(0):\n",
    "            x, hxs = self.gru(x.unsqueeze(0), (hxs * masks).unsqueeze(0))\n",
    "            x = x.squeeze(0)\n",
    "            hxs = hxs.squeeze(0)\n",
    "        else:\n",
    "            # x is a (T, N, -1) tensor that has been flatten to (T * N, -1)\n",
    "            N = hxs.size(0)\n",
    "            T = int(x.size(0) / N)\n",
    "\n",
    "            # unflatten\n",
    "            x = x.view(T, N, x.size(1))\n",
    "\n",
    "            # Same deal with masks\n",
    "            masks = masks.view(T, N)\n",
    "\n",
    "            # Let's figure out which steps in the sequence have a zero for any agent\n",
    "            # We will always assume t=0 has a zero in it as that makes the logic cleaner\n",
    "            has_zeros = ((masks[1:] == 0.0) \\\n",
    "                            .any(dim=-1)\n",
    "                            .nonzero()\n",
    "                            .squeeze()\n",
    "                            .cpu())\n",
    "\n",
    "            # +1 to correct the masks[1:]\n",
    "            if has_zeros.dim() == 0:\n",
    "                # Deal with scalar\n",
    "                has_zeros = [has_zeros.item() + 1]\n",
    "            else:\n",
    "                has_zeros = (has_zeros + 1).numpy().tolist()\n",
    "\n",
    "            # add t=0 and t=T to the list\n",
    "            has_zeros = [0] + has_zeros + [T]\n",
    "\n",
    "            hxs = hxs.unsqueeze(0)\n",
    "            outputs = []\n",
    "            for i in range(len(has_zeros) - 1):\n",
    "                # We can now process steps that don't have any zeros in masks together!\n",
    "                # This is much faster\n",
    "                start_idx = has_zeros[i]\n",
    "                end_idx = has_zeros[i + 1]\n",
    "\n",
    "                rnn_scores, hxs = self.gru(\n",
    "                    x[start_idx:end_idx],\n",
    "                    hxs * masks[start_idx].view(1, -1, 1))\n",
    "\n",
    "                outputs.append(rnn_scores)\n",
    "\n",
    "            # assert len(outputs) == T\n",
    "            # x is a (T, N, -1) tensor\n",
    "            x = torch.cat(outputs, dim=0)\n",
    "            # flatten\n",
    "            x = x.view(T * N, -1)\n",
    "            hxs = hxs.squeeze(0)\n",
    "\n",
    "        return x, hxs\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    \n",
    "class MyCNNBase(NNBase):\n",
    "    def __init__(self, num_inputs, recurrent=False, hidden_size=512):\n",
    "        super(MyCNNBase, self).__init__(recurrent, hidden_size, hidden_size)\n",
    "        \n",
    "        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "                               constant_(x, 0), nn.init.calculate_gain('relu'))\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            init_(nn.Conv2d(num_inputs, 8, 3, padding=1)), nn.ReLU(),\n",
    "            init_(nn.Conv2d(8, 16, 3, padding=1)), nn.ReLU(),\n",
    "            init_(nn.Conv2d(16, 16, 3, padding=1)), nn.ReLU(), Flatten(),\n",
    "            init_(nn.Linear(16 * 64 * 64, hidden_size)), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "                               constant_(x, 0))\n",
    "\n",
    "        self.critic_linear = init_(nn.Linear(hidden_size, 1))\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs, rnn_hxs=None, masks=None):\n",
    "        x = self.main(inputs)\n",
    "        print(x.shape)\n",
    "\n",
    "        if self.is_recurrent:\n",
    "            x, rnn_hxs = self._forward_gru(x, rnn_hxs, masks)\n",
    "\n",
    "        return self.critic_linear(x), x, rnn_hxs\n",
    "    \n",
    "model = MyCNNBase(2)\n",
    "x = torch.rand((1,2,64,64))\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /notebooks/LearningToPaint/gym-canvas\n",
    "# !pip uninstall -e .\n",
    "# !python setup.py develop -u\n",
    "\n",
    "\n",
    "import gym\n",
    "from baselines.common.vec_env.util import dict_to_obs, obs_space_info, obs_to_dict\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/notebooks/LearningToPaint/gym-canvas\")\n",
    "import gym_canvas\n",
    "\n",
    "env = gym.make('gym_canvas:canvas-v0')\n",
    "obs_space_info(env.observation_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "width = 8\n",
    "\n",
    "im = Image.new(\"L\", (width, width))\n",
    "draw = ImageDraw.Draw(im)\n",
    "draw.rectangle([2, 2, 4, 4], fill=255)\n",
    "im1 = transform(im)\n",
    "\n",
    "im = Image.new(\"L\", (width, width))\n",
    "draw = ImageDraw.Draw(im)\n",
    "draw.rectangle([2, 2, 4, 4], fill=255)\n",
    "im2 = transform(im)\n",
    "\n",
    "\n",
    "# x, y = np.random.randint(2, size=2)\n",
    "# x\n",
    "# type(y)\n",
    "\n",
    "torch.cat((im1, im2), 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import scipy.misc\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Renderer.stroke_gen import draw\n",
    "\n",
    "f = np.random.uniform(0, 1, 10)\n",
    "print(f)\n",
    "im = draw(f, 300)\n",
    "\n",
    "plt.imshow(im, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from Renderer.stroke_gen import draw_rect\n",
    "\n",
    "# width = 3\n",
    "# im = Image.new('F', (width, width))\n",
    "\n",
    "# draw = ImageDraw.Draw(im)\n",
    "# # draw.line((0, 0) + im.size, fill=128)\n",
    "# # draw.line((0, im.size[1], im.size[0], 0), fill=128)\n",
    "# draw.rectangle([1, 1, 2, 2], fill=1)\n",
    "# # plt.imshow(np.array(im), cmap='gray')\n",
    "# plt.imshow(im, vmin=0, vmax=1)\n",
    "\n",
    "im = draw_rect([0.5, 0.5, 1, 1], width=4)\n",
    "# plt.imshow(im, vmin=0, vmax=1)\n",
    "plt.imshow(1-im, cmap='gray')\n",
    "print(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_draw(draw_fn=draw_rect, n_strokes=3, width=128):\n",
    "    canvas = np.zeros((width, width, 3), dtype=int)\n",
    "    for i in range(n_strokes):\n",
    "        x = np.random.rand(4)\n",
    "        stroke = draw_fn(x, width)\n",
    "        stroke = np.expand_dims(stroke, axis=2)\n",
    "        color = np.random.randint(255, size=(3))\n",
    "        canvas = canvas * (1 - stroke) + stroke * color\n",
    "    return canvas.astype(int)\n",
    "\n",
    "plt.imshow(rand_draw(width=128))\n",
    "# plt.imshow(1-im, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Renderer.model import FCN\n",
    "\n",
    "action_dim = 4\n",
    "\n",
    "stroke_net = FCN(num_input=action_dim)\n",
    "stroke_net.load_state_dict(torch.load(\"../renderer.pkl\"))\n",
    "\n",
    "# x = torch.rand(1, action_dim)\n",
    "\n",
    "print(draw_rect([0, 0, 0.5, 0.5]))\n",
    "x = torch.from_numpy(np.array([[0, 0, 0.5, 0.5]], dtype=np.float32))\n",
    "print(x.shape)\n",
    "# x = np.random.rand(action_dim)\n",
    "stroke_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import Paint\n",
    "\n",
    "env = Paint(1, 3)\n",
    "env.load_data()\n",
    "obs = env.reset()\n",
    "obs = env.reset_with_gen()\n",
    "\n",
    "# print(obs[0, 3:6])\n",
    "im = np.array(obs[0, 3:6])\n",
    "plt.imshow(np.transpose(im, (1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Renderer.model import FCN\n",
    "from Renderer.stroke_gen import draw_rect\n",
    "# from DRL.ddpg import decode\n",
    "\n",
    "Decoder = FCN(num_input=action_dim)\n",
    "Decoder.load_state_dict(torch.load(\"../renderer.pkl\"))\n",
    "\n",
    "print(draw_rect([0, 0, 1, 1]).shape)\n",
    "\n",
    "def decode(x, canvas, width=128):\n",
    "    # x: (B * 5) * (10 + 3), 5: 5 steps, 10+3: action output\n",
    "    x = x.view(-1, action_dim + 3)\n",
    "#     stroke = 1 - Decoder(x[:, :action_dim])\n",
    "    \n",
    "    # debug: use draw_fn instead\n",
    "    stroke = []\n",
    "    for i in range(batch):\n",
    "        stroke.append(draw_rect(x[i, :action_dim]))\n",
    "    stroke = torch.tensor(stroke)\n",
    "\n",
    "    stroke = stroke.view(-1, width, width, 1)  # shape: (b, w, w, 1)\n",
    "    print(stroke.shape)\n",
    "#     color_stroke = stroke * x[:, -3:].view(-1, 1, 1, 3)\n",
    "    color_stroke = stroke * x[:, -3:].view(-1, 1, 1, 3)  # shape: (b, w, w, 3)\n",
    "#     print(color_stroke.shape)\n",
    "    stroke = stroke.permute(0, 3, 1, 2).view(-1, n_frames_per_step, 1, width, width)\n",
    "    color_stroke = color_stroke.permute(0, 3, 1, 2).view(-1, n_frames_per_step, 3, width, width)\n",
    "    for i in range(n_frames_per_step):\n",
    "        canvas = canvas * (1 - stroke[:, i]) + color_stroke[:, i]\n",
    "    return canvas\n",
    "\n",
    "n_frames_per_step = 1\n",
    "batch = 1\n",
    "width = 128\n",
    "\n",
    "canvas = torch.zeros(batch,3,width,width)# [B x 3 x width x width]\n",
    "\n",
    "_action = torch.rand(batch, 4)\n",
    "color = torch.rand(batch, 3)\n",
    "action = torch.cat((_action, color), 1)\n",
    "\n",
    "canvas_out =  decode(action, canvas, width)\n",
    "\n",
    "im = canvas_out.squeeze(0).detach().numpy()\n",
    "im = np.transpose(im, (1, 2, 0))\n",
    "# canvas = decode(action, canvas)\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import Paint\n",
    "from DRL.ddpg import DDPG\n",
    "from utils.util import to_numpy\n",
    "from utils.tensorboard import TensorBoard\n",
    "\n",
    "exp = os.path.abspath(\".\").split(\"/\")[-1]\n",
    "writer = TensorBoard(\"../train_log/{}\".format(exp))\n",
    "\n",
    "agent = DDPG(1, 1, 5, 0.001, 0.95 ** 5, 800, writer, None, \"./model\")\n",
    "\n",
    "env = Paint(1, 5)\n",
    "obs = env.reset_with_gen()\n",
    "agent.reset(obs, 0)\n",
    "action = agent.select_action(obs, noise_factor=0)\n",
    "obs, reward, done, _ = env.step(torch.tensor(action))\n",
    "# env.step()\n",
    "\n",
    "# print(obs.shape)\n",
    "# env.gt[0].cpu().data.numpy()\n",
    "cv = to_numpy(env.canvas[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = [np.random.rand(4) for _ in range(3)]\n",
    "# x = np.stack(x)\n",
    "# print(x)\n",
    "# # x.shape\n",
    "\n",
    "# a = np.random.rand(3)\n",
    "# print(a)\n",
    "# b = np.random.rand(4)\n",
    "# np.concatenate((a,b))\n",
    "x = torch.tensor([1,2,3,4])\n",
    "x.view(2, -1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

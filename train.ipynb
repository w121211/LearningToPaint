{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker\n",
    "\n",
    "```bash\n",
    "sudo docker build -t maskrcnn-benchmark .\n",
    "sudo docker run \n",
    "    --runtime=nvidia -d -it \\\n",
    "    --name=maskrcnn \\\n",
    "    -v=$(pwd):/notebooks \\\n",
    "    -p=8888:8888 \\\n",
    "    --ipc=\"host\" \\\n",
    "    maskrcnn-benchmark\n",
    "sudo docker logs maskrcnn\n",
    "sudo docker start maskrcnn\n",
    "sudo docker exec -it maskrcnn /bin/bash\n",
    "```\n",
    "\n",
    "## Notes\n",
    "\n",
    "model-free vs model-based: model包含actor & critic，在critic方面，model-based有含renderer net, model-free沒有"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/LearningToPaint/rllib\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0831 19:14:11.173088 140287925675776 deprecation.py:323] From /miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2019-08-31 19:14:11,744\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-08-31_19-14-11_743767_16823/logs.\n",
      "2019-08-31 19:14:11,858\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:60571 to respond...\n",
      "2019-08-31 19:14:12,001\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:62276 to respond...\n",
      "2019-08-31 19:14:12,005\tINFO services.py:809 -- Starting Redis shard with 1.67 GB max memory.\n",
      "2019-08-31 19:14:12,050\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-08-31_19-14-11_743767_16823/logs.\n",
      "2019-08-31 19:14:12,052\tINFO services.py:1475 -- Starting the Plasma object store with 2.51 GB memory using /dev/shm.\n",
      "2019-08-31 19:14:12,288\tINFO trial_runner.py:176 -- Starting a new experiment.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/2 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 2.8/8.4 GB\n",
      "\n",
      "2019-08-31 19:14:12,327\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "W0831 19:14:12.386737 140287925675776 deprecation_wrapper.py:119] From /miniconda/envs/py36/lib/python3.6/site-packages/ray/tune/logger.py:133: The name tf.VERSION is deprecated. Please use tf.version.VERSION instead.\n",
      "\n",
      "W0831 19:14:12.392090 140287925675776 deprecation_wrapper.py:119] From /miniconda/envs/py36/lib/python3.6/site-packages/ray/tune/logger.py:138: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "2019-08-31 19:14:12,407\tERROR log_sync.py:34 -- Log sync requires cluster to be setup with `ray up`.\n",
      "2019-08-31 19:14:12,561\tWARNING util.py:145 -- The `start_trial` operation took 0.2452104091644287 seconds to complete, which may be a performance bottleneck.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 2.8/8.4 GB\n",
      "Result logdir: /root/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CanvasEnv_0:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m W0831 19:14:16.679027 140528825677568 deprecation.py:323] From /miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:17,212\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:17.213798: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:17.221146: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1296000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:17.221492: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5562f4e472b0 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:17.221536: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m W0831 19:14:17.237014 140528825677568 deprecation.py:506] From /miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m Tensor(\"default_policy/Reshape:0\", shape=(?, 9, 9, 1), dtype=float32)\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m W0831 19:14:17.718589 140528825677568 deprecation.py:323] From /miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:77: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m Use `tf.random.categorical` instead.\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m W0831 19:14:17.756308 140528825677568 deprecation.py:323] From /miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:17.885224: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:18,015\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 166) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 166) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m W0831 19:14:18.090119 140528825677568 deprecation.py:506] From /miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m W0831 19:14:18.098865 140528825677568 deprecation.py:506] From /miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m W0831 19:14:18.168618 140528825677568 deprecation.py:323] From /miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:21,010\tINFO rollout_worker.py:742 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fcee63ca438>}\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:21,010\tINFO rollout_worker.py:743 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.DictFlatteningPreprocessor object at 0x7fcee63d5dd8>}\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:21,010\tINFO rollout_worker.py:356 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fcee63d5898>}\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:21,070\tINFO multi_gpu_optimizer.py:93 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m Tensor(\"default_policy_1/tower/Reshape:0\", shape=(?, 9, 9, 1), dtype=float32)\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m Tensor(\"default_policy_1/tower_1/Reshape:0\", shape=(?, 9, 9, 1), dtype=float32, device=/device:CPU:0)\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m W0831 19:14:25.146265 139986820921088 deprecation.py:323] From /miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m 2019-08-31 19:14:25,862\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m 2019-08-31 19:14:25.885915: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m 2019-08-31 19:14:25.897907: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1296000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m 2019-08-31 19:14:25.898281: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bd852cc390 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m 2019-08-31 19:14:25.898329: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m W0831 19:14:25.929115 139986820921088 deprecation.py:506] From /miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m Tensor(\"default_policy/Reshape:0\", shape=(?, 9, 9, 1), dtype=float32)\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m W0831 19:14:27.089614 139986820921088 deprecation.py:323] From /miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:77: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m Use `tf.random.categorical` instead.\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m W0831 19:14:27.121941 139986820921088 deprecation.py:323] From /miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:138: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m 2019-08-31 19:14:27.267026: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m 2019-08-31 19:14:27,514\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 166) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 166) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m W0831 19:14:27.800016 139986820921088 deprecation.py:506] From /miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m W0831 19:14:27.820863 139986820921088 deprecation.py:506] From /miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m W0831 19:14:27.983448 139986820921088 deprecation.py:323] From /miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:31,437\tINFO trainable.py:105 -- _setup took 14.244 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m W0831 19:14:31.441348 140528825677568 deprecation.py:323] From /miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m W0831 19:14:31.597239 139986820921088 deprecation.py:323] From /miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m 2019-08-31 19:14:33,777\tINFO rollout_worker.py:451 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m 2019-08-31 19:14:33,779\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent0': { 'cur_im': np.ndarray((9, 9, 1), dtype=float32, min=0.0, max=1.0, mean=0.198),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                    'obj_status': np.ndarray((1, 4), dtype=float32, min=0.0, max=0.333, mean=0.167),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                    'target_im': np.ndarray((9, 9, 1), dtype=float32, min=0.0, max=1.0, mean=0.198)}}}\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m 2019-08-31 19:14:33,779\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m 2019-08-31 19:14:33,780\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((166,), dtype=float64, min=0.0, max=1.0, mean=0.197)\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m 2019-08-31 19:14:33,780\tINFO sampler.py:407 -- Filtered obs: np.ndarray((166,), dtype=float64, min=0.0, max=1.0, mean=0.197)\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m 2019-08-31 19:14:33,781\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                                   'obs': np.ndarray((166,), dtype=float64, min=0.0, max=1.0, mean=0.197),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                                   'prev_action': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m 2019-08-31 19:14:33,782\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m 2019-08-31 19:14:33,903\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m { 'default_policy': ( { 'data': { 'batches': [ np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                                                np.ndarray((1, 2), dtype=float32, min=0.089, max=1.518, mean=0.803)]},\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'type': 'TupleActions'},\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.051, max=0.051, mean=0.051),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'behaviour_logits': np.ndarray((1, 5), dtype=float32, min=-0.019, max=0.037, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.004, max=0.004, mean=0.004)})}\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m 2019-08-31 19:14:34,812\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.003, max=0.204, mean=0.088),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'actions': np.ndarray((200, 3), dtype=float32, min=-2.728, max=2.61, mean=-0.03),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'advantages': np.ndarray((200,), dtype=float32, min=-5063.782, max=-35.061, mean=-2833.143),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'behaviour_logits': np.ndarray((200, 5), dtype=float32, min=-0.256, max=0.361, mean=-0.012),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'eps_id': np.ndarray((200,), dtype=int64, min=193679466.0, max=193679466.0, mean=193679466.0),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'infos': np.ndarray((200,), dtype=object, head={'episode': {'r': -161.79588431641605}}),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'new_obs': np.ndarray((200, 166), dtype=float32, min=0.0, max=2.944, mean=0.175),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'obs': np.ndarray((200, 166), dtype=float32, min=0.0, max=2.944, mean=0.175),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'prev_actions': np.ndarray((200, 3), dtype=float32, min=-2.728, max=2.61, mean=-0.031),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'prev_rewards': np.ndarray((200,), dtype=float32, min=-509.936, max=0.0, mean=-51.056),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'rewards': np.ndarray((200,), dtype=float32, min=-509.936, max=-0.285, mean=-51.231),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         't': np.ndarray((200,), dtype=int64, min=0.0, max=199.0, mean=99.5),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'value_targets': np.ndarray((200,), dtype=float32, min=-5063.737, max=-34.946, mean=-2833.077),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m                         'vf_preds': np.ndarray((200,), dtype=float32, min=0.004, max=0.35, mean=0.066)},\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m 2019-08-31 19:14:34,817\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.003, max=0.204, mean=0.088),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m             'actions': np.ndarray((200, 3), dtype=float32, min=-2.728, max=2.61, mean=-0.03),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=-5063.782, max=-35.061, mean=-2833.143),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m             'behaviour_logits': np.ndarray((200, 5), dtype=float32, min=-0.256, max=0.361, mean=-0.012),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=193679466.0, max=193679466.0, mean=193679466.0),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={'episode': {'r': -161.79588431641605}}),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m             'new_obs': np.ndarray((200, 166), dtype=float32, min=0.0, max=2.944, mean=0.175),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m             'obs': np.ndarray((200, 166), dtype=float32, min=0.0, max=2.944, mean=0.175),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m             'prev_actions': np.ndarray((200, 3), dtype=float32, min=-2.728, max=2.61, mean=-0.031),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-509.936, max=0.0, mean=-51.056),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=-509.936, max=-0.285, mean=-51.231),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=199.0, mean=99.5),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=-5063.737, max=-34.946, mean=-2833.077),\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=0.004, max=0.35, mean=0.066)},\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=16851)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,179\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/conv2d/kernel:0' shape=(3, 3, 1, 8) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,179\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/conv2d/bias:0' shape=(8,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,181\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/conv2d_1/kernel:0' shape=(3, 3, 8, 32) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,181\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/conv2d_1/bias:0' shape=(32,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,181\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/conv2d_2/kernel:0' shape=(3, 3, 32, 64) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,181\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/conv2d_2/bias:0' shape=(64,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,182\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/conv2d_3/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,182\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/conv2d_3/bias:0' shape=(64,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,182\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/conv2d_4/kernel:0' shape=(3, 3, 64, 1) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,182\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/conv2d_4/bias:0' shape=(1,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,182\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/dense/kernel:0' shape=(166, 128) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,183\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/dense/bias:0' shape=(128,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,183\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/dense_1/kernel:0' shape=(128, 128) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,183\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/dense_1/bias:0' shape=(128,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,183\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/dense_2/kernel:0' shape=(128, 5) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,184\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/dense_2/bias:0' shape=(5,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,184\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/dense_3/kernel:0' shape=(128, 1) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,184\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/dense_3/bias:0' shape=(1,) dtype=float32>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,199\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m { 'inputs': [ np.ndarray((4000, 3), dtype=float32, min=-3.735, max=3.655, mean=-0.02),\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-1146.852, max=0.0, mean=-57.455),\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m               np.ndarray((4000, 166), dtype=float32, min=0.0, max=3.988, mean=0.174),\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m               np.ndarray((4000, 3), dtype=float32, min=-3.735, max=3.655, mean=-0.02),\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-1.982, max=2.208, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m               np.ndarray((4000, 5), dtype=float32, min=-0.419, max=0.527, mean=-0.01),\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-6157.924, max=-4.625, mean=-3247.304),\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.003, max=0.532, mean=0.073)],\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 166) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:14:53,199\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n",
      "Result for PPO_CanvasEnv_0:\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m /miniconda/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m   out=out, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m /miniconda/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "  custom_metrics: {}\n",
      "  date: 2019-08-31_19-20-51\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 277b6bc2145d488d95646f9eff06263e\n",
      "  hostname: 714a8423a409\n",
      "  info:\n",
      "    grad_time_ms: 358268.245\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3510894775390625\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3477431535720825\n",
      "        policy_loss: 0.2988738715648651\n",
      "        total_loss: 12638025.0\n",
      "        vf_explained_var: 5.456324652186595e-05\n",
      "        vf_loss: 12638024.0\n",
      "    load_time_ms: 301.27\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 3968\n",
      "    sample_time_ms: 19478.966\n",
      "    update_time_ms: 2101.085\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  pid: 16852\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 380.3666715621948\n",
      "  time_this_iter_s: 380.3666715621948\n",
      "  time_total_s: 380.3666715621948\n",
      "  timestamp: 1567279251\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 8410720a\n",
      "  \n",
      "W0831 19:20:51.998038 140287925675776 deprecation_wrapper.py:119] From /miniconda/envs/py36/lib/python3.6/site-packages/ray/tune/logger.py:118: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 3.7/8.4 GB\n",
      "Result logdir: /root/ray_results/PPO\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_CanvasEnv_0:\tRUNNING, [2 CPUs, 0 GPUs], [pid=16852], 380 s, 1 iter, 4000 ts, nan rew\n",
      "\n",
      "2019-08-31 19:20:53,229\tERROR trial_runner.py:550 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 498, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 342, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/worker.py\", line 2247, in get\n",
      "    raise value\n",
      "ray.exceptions.RayTaskError: \u001b[36mray_PPO:train()\u001b[39m (pid=16852, host=714a8423a409)\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 369, in train\n",
      "    raise e\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 358, in train\n",
      "    result = Trainable.train(self)\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/tune/trainable.py\", line 171, in train\n",
      "    result = self._train()\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 126, in _train\n",
      "    fetches = self.optimizer.step()\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/optimizers/multi_gpu_optimizer.py\", line 140, in step\n",
      "    self.num_envs_per_worker, self.train_batch_size)\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/optimizers/rollout.py\", line 29, in collect_samples\n",
      "    next_sample = ray_get_and_free(fut_sample)\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/utils/memory.py\", line 33, in ray_get_and_free\n",
      "    result = ray.get(object_ids)\n",
      "ray.exceptions.RayTaskError: \u001b[36mray_RolloutWorker:sample()\u001b[39m (pid=16851, host=714a8423a409)\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 453, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 56, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 97, in get_data\n",
      "    item = next(self.rollout_provider)\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 313, in _env_runner\n",
      "    soft_horizon)\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/evaluation/sampler.py\", line 401, in _process_observations\n",
      "    policy_id).transform(raw_obs)\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/models/preprocessors.py\", line 229, in transform\n",
      "    self.check_shape(observation)\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/rllib/models/preprocessors.py\", line 61, in check_shape\n",
      "    self._obs_space, observation)\n",
      "ValueError: ('Observation outside expected value range', Dict(cur_im:Box(9, 9, 1), obj_status:Box(1, 4), target_im:Box(9, 9, 1)), {'target_im': array([[[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]],\n",
      "\n",
      "       [[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]],\n",
      "\n",
      "       [[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]],\n",
      "\n",
      "       [[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]],\n",
      "\n",
      "       [[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]],\n",
      "\n",
      "       [[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]],\n",
      "\n",
      "       [[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]],\n",
      "\n",
      "       [[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]],\n",
      "\n",
      "       [[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]], dtype=float32), 'cur_im': array([[[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]],\n",
      "\n",
      "       [[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]],\n",
      "\n",
      "       [[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]],\n",
      "\n",
      "       [[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]],\n",
      "\n",
      "       [[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]],\n",
      "\n",
      "       [[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]],\n",
      "\n",
      "       [[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]],\n",
      "\n",
      "       [[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]],\n",
      "\n",
      "       [[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]], dtype=float32), 'obj_status': array([[5.2450967, 0.7338051, 5.57843  , 1.0671384]], dtype=float32)})\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/2 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 3.7/8.4 GB\n",
      "Result logdir: /root/ray_results/PPO\n",
      "Number of trials: 1 ({'ERROR': 1})\n",
      "ERROR trials:\n",
      " - PPO_CanvasEnv_0:\tERROR, 1 failures: /root/ray_results/PPO/PPO_CanvasEnv_0_2019-08-31_19-14-12psp81ut0/error_2019-08-31_19-20-53.txt, [2 CPUs, 0 GPUs], [pid=16852], 380 s, 1 iter, 4000 ts, nan rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16852)\u001b[0m 2019-08-31 19:20:53,211\tINFO trainer.py:366 -- Worker crashed during call to train(). To attempt to continue training without the failed worker, set `'ignore_worker_failures': True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"canvas_env.py\", line 180, in <module>\r\n",
      "    \"num_workers\": 1,  # parallelism\r\n",
      "  File \"/miniconda/envs/py36/lib/python3.6/site-packages/ray/tune/tune.py\", line 262, in run\r\n",
      "    raise TuneError(\"Trials did not complete\", errored_trials)\r\n",
      "ray.tune.error.TuneError: ('Trials did not complete', [PPO_CanvasEnv_0])\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install ray \n",
    "# ray[rllib] ray[debug] pandas\n",
    "\n",
    "%cd /notebooks/LearningToPaint/rllib\n",
    "!python canvas_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install openai baselines and other required packages\n",
    "# !pip install gym tensorflow\n",
    "\n",
    "# !apt-get update && apt-get -y install cmake libopenmpi-dev python3-dev zlib1g-dev\n",
    "# %cd /notebooks/baselines\n",
    "# # !git clone https://github.com/openai/baselines.git  # use github-desktop to clone repository\n",
    "# !pip install -e .\n",
    "\n",
    "# # install local gym-canvas as package\n",
    "# %cd /notebooks/LearningToPaint/gym-canvas\n",
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d2mAkgRjwwuf"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/LearningToPaint/pytorch-a2c-ppo-acktr-gail\n",
      "Tuple\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 254, in <module>\n",
      "    main()\n",
      "  File \"main.py\", line 55, in main\n",
      "    base_kwargs={\"recurrent\": args.recurrent_policy},\n",
      "  File \"/notebooks/LearningToPaint/pytorch-a2c-ppo-acktr-gail/a2c_ppo_acktr/model.py\", line 41, in __init__\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n"
     ]
    }
   ],
   "source": [
    "%cd /notebooks/LearningToPaint/pytorch-a2c-ppo-acktr-gail\n",
    "!python main.py --env-name \"gym_canvas:canvas-v0\" --num-processes 1 --algo ppo --use-gae --lr 2.5e-4 --clip-param 0.1 --value-loss-coef 0.5 --num-steps 5 --num-mini-batch 4 --log-interval 1 --use-linear-lr-decay --entropy-coef 0.01\n",
    "\n",
    "# !python baseline/train_renderer.py\n",
    "# !tensorboard --logdir train_log --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train RL\n",
    "\n",
    "* make data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory './model': File exists\n",
      "Renderer model loaded.\n",
      "observation_space (1, 128, 128, 7) action_space 13\n",
      "\u001b[98m #0: steps:4 interval_time:0.22 train_time:24.56\u001b[00m\n",
      "evaluating\n",
      "\u001b[91m Step_0000007: mean_reward:0.288 mean_dist:0.201 var_dist:0.000\u001b[00m\n",
      "\u001b[98m #1: steps:8 interval_time:2.69 train_time:34.23\u001b[00m\n",
      "evaluating\n",
      "\u001b[91m Step_0000011: mean_reward:-8.902 mean_dist:0.619 var_dist:0.000\u001b[00m\n",
      "\u001b[98m #2: steps:12 interval_time:2.55 train_time:43.24\u001b[00m\n",
      "evaluating\n",
      "\u001b[91m Step_0000015: mean_reward:-7.753 mean_dist:0.417 var_dist:0.000\u001b[00m\n",
      "\u001b[98m #3: steps:16 interval_time:2.68 train_time:55.46\u001b[00m\n",
      "evaluating\n",
      "\u001b[91m Step_0000019: mean_reward:-1.631 mean_dist:0.326 var_dist:0.000\u001b[00m\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python baseline/train.py --max_step=4 --env_batch=1 --warmup=1 --validate_interval=1 --debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renderer model loaded.\n",
      "step 0: loss 9167.975586\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"baseline/train_supervised.py\", line 84, in <module>\n",
      "    train(args.batch_size, args.episode_steps)"
     ]
    }
   ],
   "source": [
    "!python baseline/train_supervised.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "learningtopaint.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
